{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from datetime import datetime as dt\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "import scipy\n",
    "import seaborn\n",
    "import jieba\n",
    "# jieba.enable_parallel(4)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryRating(rating):\n",
    "    return 1 if rating > 3 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = MongoClient('10.160.1.15:27017')['googleplay']['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(collection.find({},{'comments':1,'ratings':1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"google_play.csv\")\n",
    "df = pd.read_csv(\"google_play.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['comments','ratings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = df.apply(lambda x:len(str(x['comments']).strip()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(wordCount>=4) & (df['ratings']!=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.548 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.548 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "cut = df[['comments']].apply(lambda x: \" \".join(jieba.cut(x['comments'].strip() + \" $ENDING$\",cut_all=False)),axis=1)\n",
    "# cut = df[['comments']].apply(lambda x: \" \".join(x['comments']) + \" $ENDING$\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_corpus = WikiCorpus('zhwiki-20190320-pages-articles-multistream.xml.bz2', dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已處理 1000 篇文章\n",
      "已處理 2000 篇文章\n",
      "已處理 3000 篇文章\n",
      "已處理 4000 篇文章\n",
      "已處理 5000 篇文章\n",
      "已處理 6000 篇文章\n",
      "已處理 7000 篇文章\n",
      "已處理 8000 篇文章\n",
      "已處理 9000 篇文章\n",
      "已處理 10000 篇文章\n",
      "已處理 11000 篇文章\n",
      "已處理 12000 篇文章\n",
      "已處理 13000 篇文章\n",
      "已處理 14000 篇文章\n",
      "已處理 15000 篇文章\n",
      "已處理 16000 篇文章\n",
      "已處理 17000 篇文章\n",
      "已處理 18000 篇文章\n",
      "已處理 19000 篇文章\n",
      "已處理 20000 篇文章\n",
      "已處理 21000 篇文章\n",
      "已處理 22000 篇文章\n",
      "已處理 23000 篇文章\n",
      "已處理 24000 篇文章\n",
      "已處理 25000 篇文章\n",
      "已處理 26000 篇文章\n",
      "已處理 27000 篇文章\n",
      "已處理 28000 篇文章\n",
      "已處理 29000 篇文章\n",
      "已處理 30000 篇文章\n",
      "已處理 31000 篇文章\n",
      "已處理 32000 篇文章\n",
      "已處理 33000 篇文章\n",
      "已處理 34000 篇文章\n",
      "已處理 35000 篇文章\n",
      "已處理 36000 篇文章\n",
      "已處理 37000 篇文章\n",
      "已處理 38000 篇文章\n",
      "已處理 39000 篇文章\n",
      "已處理 40000 篇文章\n",
      "已處理 41000 篇文章\n",
      "已處理 42000 篇文章\n",
      "已處理 43000 篇文章\n",
      "已處理 44000 篇文章\n",
      "已處理 45000 篇文章\n",
      "已處理 46000 篇文章\n",
      "已處理 47000 篇文章\n",
      "已處理 48000 篇文章\n",
      "已處理 49000 篇文章\n",
      "已處理 50000 篇文章\n",
      "已處理 51000 篇文章\n",
      "已處理 52000 篇文章\n",
      "已處理 53000 篇文章\n",
      "已處理 54000 篇文章\n",
      "已處理 55000 篇文章\n",
      "已處理 56000 篇文章\n",
      "已處理 57000 篇文章\n",
      "已處理 58000 篇文章\n",
      "已處理 59000 篇文章\n",
      "已處理 60000 篇文章\n",
      "已處理 61000 篇文章\n",
      "已處理 62000 篇文章\n",
      "已處理 63000 篇文章\n",
      "已處理 64000 篇文章\n",
      "已處理 65000 篇文章\n",
      "已處理 66000 篇文章\n",
      "已處理 67000 篇文章\n",
      "已處理 68000 篇文章\n",
      "已處理 69000 篇文章\n",
      "已處理 70000 篇文章\n",
      "已處理 71000 篇文章\n",
      "已處理 72000 篇文章\n",
      "已處理 73000 篇文章\n",
      "已處理 74000 篇文章\n",
      "已處理 75000 篇文章\n",
      "已處理 76000 篇文章\n",
      "已處理 77000 篇文章\n",
      "已處理 78000 篇文章\n",
      "已處理 79000 篇文章\n",
      "已處理 80000 篇文章\n",
      "已處理 81000 篇文章\n",
      "已處理 82000 篇文章\n",
      "已處理 83000 篇文章\n",
      "已處理 84000 篇文章\n",
      "已處理 85000 篇文章\n",
      "已處理 86000 篇文章\n",
      "已處理 87000 篇文章\n",
      "已處理 88000 篇文章\n",
      "已處理 89000 篇文章\n",
      "已處理 90000 篇文章\n",
      "已處理 91000 篇文章\n",
      "已處理 92000 篇文章\n",
      "已處理 93000 篇文章\n",
      "已處理 94000 篇文章\n",
      "已處理 95000 篇文章\n",
      "已處理 96000 篇文章\n",
      "已處理 97000 篇文章\n",
      "已處理 98000 篇文章\n",
      "已處理 99000 篇文章\n",
      "已處理 100000 篇文章\n",
      "已處理 101000 篇文章\n",
      "已處理 102000 篇文章\n",
      "已處理 103000 篇文章\n",
      "已處理 104000 篇文章\n",
      "已處理 105000 篇文章\n",
      "已處理 106000 篇文章\n",
      "已處理 107000 篇文章\n",
      "已處理 108000 篇文章\n",
      "已處理 109000 篇文章\n",
      "已處理 110000 篇文章\n",
      "已處理 111000 篇文章\n",
      "已處理 112000 篇文章\n",
      "已處理 113000 篇文章\n",
      "已處理 114000 篇文章\n",
      "已處理 115000 篇文章\n",
      "已處理 116000 篇文章\n",
      "已處理 117000 篇文章\n",
      "已處理 118000 篇文章\n",
      "已處理 119000 篇文章\n",
      "已處理 120000 篇文章\n",
      "已處理 121000 篇文章\n",
      "已處理 122000 篇文章\n",
      "已處理 123000 篇文章\n",
      "已處理 124000 篇文章\n",
      "已處理 125000 篇文章\n",
      "已處理 126000 篇文章\n",
      "已處理 127000 篇文章\n",
      "已處理 128000 篇文章\n",
      "已處理 129000 篇文章\n",
      "已處理 130000 篇文章\n",
      "已處理 131000 篇文章\n",
      "已處理 132000 篇文章\n",
      "已處理 133000 篇文章\n",
      "已處理 134000 篇文章\n",
      "已處理 135000 篇文章\n",
      "已處理 136000 篇文章\n",
      "已處理 137000 篇文章\n",
      "已處理 138000 篇文章\n",
      "已處理 139000 篇文章\n",
      "已處理 140000 篇文章\n",
      "已處理 141000 篇文章\n",
      "已處理 142000 篇文章\n",
      "已處理 143000 篇文章\n",
      "已處理 144000 篇文章\n",
      "已處理 145000 篇文章\n",
      "已處理 146000 篇文章\n",
      "已處理 147000 篇文章\n",
      "已處理 148000 篇文章\n",
      "已處理 149000 篇文章\n",
      "已處理 150000 篇文章\n",
      "已處理 151000 篇文章\n",
      "已處理 152000 篇文章\n",
      "已處理 153000 篇文章\n",
      "已處理 154000 篇文章\n",
      "已處理 155000 篇文章\n",
      "已處理 156000 篇文章\n",
      "已處理 157000 篇文章\n",
      "已處理 158000 篇文章\n",
      "已處理 159000 篇文章\n",
      "已處理 160000 篇文章\n",
      "已處理 161000 篇文章\n",
      "已處理 162000 篇文章\n",
      "已處理 163000 篇文章\n",
      "已處理 164000 篇文章\n",
      "已處理 165000 篇文章\n",
      "已處理 166000 篇文章\n",
      "已處理 167000 篇文章\n",
      "已處理 168000 篇文章\n",
      "已處理 169000 篇文章\n",
      "已處理 170000 篇文章\n",
      "已處理 171000 篇文章\n",
      "已處理 172000 篇文章\n",
      "已處理 173000 篇文章\n",
      "已處理 174000 篇文章\n",
      "已處理 175000 篇文章\n",
      "已處理 176000 篇文章\n",
      "已處理 177000 篇文章\n",
      "已處理 178000 篇文章\n",
      "已處理 179000 篇文章\n",
      "已處理 180000 篇文章\n",
      "已處理 181000 篇文章\n",
      "已處理 182000 篇文章\n",
      "已處理 183000 篇文章\n",
      "已處理 184000 篇文章\n",
      "已處理 185000 篇文章\n",
      "已處理 186000 篇文章\n",
      "已處理 187000 篇文章\n",
      "已處理 188000 篇文章\n",
      "已處理 189000 篇文章\n",
      "已處理 190000 篇文章\n",
      "已處理 191000 篇文章\n",
      "已處理 192000 篇文章\n",
      "已處理 193000 篇文章\n",
      "已處理 194000 篇文章\n",
      "已處理 195000 篇文章\n",
      "已處理 196000 篇文章\n",
      "已處理 197000 篇文章\n",
      "已處理 198000 篇文章\n",
      "已處理 199000 篇文章\n",
      "已處理 200000 篇文章\n",
      "已處理 201000 篇文章\n",
      "已處理 202000 篇文章\n",
      "已處理 203000 篇文章\n",
      "已處理 204000 篇文章\n",
      "已處理 205000 篇文章\n",
      "已處理 206000 篇文章\n",
      "已處理 207000 篇文章\n",
      "已處理 208000 篇文章\n",
      "已處理 209000 篇文章\n",
      "已處理 210000 篇文章\n",
      "已處理 211000 篇文章\n",
      "已處理 212000 篇文章\n",
      "已處理 213000 篇文章\n",
      "已處理 214000 篇文章\n",
      "已處理 215000 篇文章\n",
      "已處理 216000 篇文章\n",
      "已處理 217000 篇文章\n",
      "已處理 218000 篇文章\n",
      "已處理 219000 篇文章\n",
      "已處理 220000 篇文章\n",
      "已處理 221000 篇文章\n",
      "已處理 222000 篇文章\n",
      "已處理 223000 篇文章\n",
      "已處理 224000 篇文章\n",
      "已處理 225000 篇文章\n",
      "已處理 226000 篇文章\n",
      "已處理 227000 篇文章\n",
      "已處理 228000 篇文章\n",
      "已處理 229000 篇文章\n",
      "已處理 230000 篇文章\n",
      "已處理 231000 篇文章\n",
      "已處理 232000 篇文章\n",
      "已處理 233000 篇文章\n",
      "已處理 234000 篇文章\n",
      "已處理 235000 篇文章\n",
      "已處理 236000 篇文章\n",
      "已處理 237000 篇文章\n",
      "已處理 238000 篇文章\n",
      "已處理 239000 篇文章\n",
      "已處理 240000 篇文章\n",
      "已處理 241000 篇文章\n",
      "已處理 242000 篇文章\n",
      "已處理 243000 篇文章\n"
     ]
    }
   ],
   "source": [
    "texts_num = 0\n",
    "with open(\"wiki_zh_tw.txt\",'w',encoding='utf-8') as output:\n",
    "    for text in df['comments']:\n",
    "        output.write(text + '\\n')\n",
    "        texts_num += 1\n",
    "        if texts_num % 1000 == 0:\n",
    "            print(\"已處理 %d 篇文章\" % texts_num)\n",
    "    output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成前 10000 行的斷詞\n",
      "已完成前 20000 行的斷詞\n",
      "已完成前 30000 行的斷詞\n",
      "已完成前 40000 行的斷詞\n",
      "已完成前 50000 行的斷詞\n",
      "已完成前 60000 行的斷詞\n",
      "已完成前 70000 行的斷詞\n",
      "已完成前 80000 行的斷詞\n",
      "已完成前 90000 行的斷詞\n",
      "已完成前 100000 行的斷詞\n",
      "已完成前 110000 行的斷詞\n",
      "已完成前 120000 行的斷詞\n",
      "已完成前 130000 行的斷詞\n",
      "已完成前 140000 行的斷詞\n",
      "已完成前 150000 行的斷詞\n",
      "已完成前 160000 行的斷詞\n",
      "已完成前 170000 行的斷詞\n",
      "已完成前 180000 行的斷詞\n",
      "已完成前 190000 行的斷詞\n",
      "已完成前 200000 行的斷詞\n",
      "已完成前 210000 行的斷詞\n",
      "已完成前 220000 行的斷詞\n",
      "已完成前 230000 行的斷詞\n",
      "已完成前 240000 行的斷詞\n"
     ]
    }
   ],
   "source": [
    "output = open('wiki_seg.txt', 'w', encoding='utf-8')\n",
    "with open('wiki_zh_tw.txt', 'r', encoding='utf-8') as content :\n",
    "    for texts_num, line in enumerate(content):\n",
    "        line = line.strip('\\n')\n",
    "        words = jieba.cut(line, cut_all=False)\n",
    "        for word in words:\n",
    "#             if word not in stopword_set:\n",
    "            output.write(word + ' ')\n",
    "        output.write('\\n')\n",
    "\n",
    "        if (texts_num + 1) % 10000 == 0:\n",
    "            print(\"已完成前 %d 行的斷詞\" % (texts_num + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "WORD_VECTOR_DIM = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "# Create a model to represent each word by a 10 dimensional vector.\n",
    "sentences = models.word2vec.Text8Corpus(cut[0])\n",
    "\n",
    "sentences = word2vec.LineSentence(\"wiki_seg.txt\")\n",
    "# wvmodel = word2vec.Word2Vec(sentences, size=250)\n",
    "wvmodel = Word2Vec(sentences, size=WORD_VECTOR_DIM, window=5, min_count=0, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, Flatten, Dropout, GRU, BatchNormalization, Bidirectional, Input\n",
    "from keras.layers import Embedding, Concatenate, RepeatVector, Permute, Multiply, Lambda, Dot, Conv2D, MaxPooling2D, Reshape\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 135685 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=1000000000)\n",
    "tokenizer.fit_on_texts(cut)\n",
    "sequences = tokenizer.texts_to_sequences(cut)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141327 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for word in wvmodel.wv.vocab:\n",
    "    coefs = np.asarray(wvmodel.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, WORD_VECTOR_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            WORD_VECTOR_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENTENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['ratings'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i] = 1 if y[i] > 3 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162186 samples, validate on 81094 samples\n",
      "Epoch 1/15\n",
      "162186/162186 [==============================] - 27s 169us/step - loss: 0.4465 - acc: 0.7955 - val_loss: 0.3235 - val_acc: 0.8582\n",
      "Epoch 2/15\n",
      "162186/162186 [==============================] - 22s 133us/step - loss: 0.3199 - acc: 0.8592 - val_loss: 0.3049 - val_acc: 0.8666\n",
      "Epoch 3/15\n",
      "162186/162186 [==============================] - 20s 124us/step - loss: 0.3078 - acc: 0.8642 - val_loss: 0.2982 - val_acc: 0.8698\n",
      "Epoch 4/15\n",
      "162186/162186 [==============================] - 21s 129us/step - loss: 0.3004 - acc: 0.8686 - val_loss: 0.2955 - val_acc: 0.8717\n",
      "Epoch 5/15\n",
      "162186/162186 [==============================] - 21s 127us/step - loss: 0.2956 - acc: 0.8713 - val_loss: 0.2909 - val_acc: 0.8738\n",
      "Epoch 6/15\n",
      "162186/162186 [==============================] - 21s 132us/step - loss: 0.2913 - acc: 0.8728 - val_loss: 0.2872 - val_acc: 0.8749\n",
      "Epoch 7/15\n",
      " 82944/162186 [==============>...............] - ETA: 9s - loss: 0.2870 - acc: 0.8750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-279-30b2a410704e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stratified_folder = StratifiedKFold(n_splits=3, random_state=0, shuffle=False) \n",
    "for train_index, test_index in stratified_folder.split(data, y):\n",
    "    \n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "        \n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    embedded_layer = embedding_layer(input_layer)\n",
    "    \n",
    "    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(32, return_sequences=True, return_state=True, activation='tanh'))(embedded_layer)\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    \n",
    "    attention = Dense(50, activation='softmax')(forward_h)\n",
    "#     attention = Flatten()(attention)\n",
    "#     attention = Activation('softmax')(attention)\n",
    "#     attention = RepeatVector(128)(attention)\n",
    "    lstm = Permute([2, 1])(lstm)\n",
    "\n",
    "    sent_representation = Multiply()([lstm,attention])\n",
    "#     sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(32,))(sent_representation)\n",
    "    \n",
    "    hidden_layer = Flatten()(sent_representation)\n",
    "\n",
    "    hidden_layer = Dropout(0.5)(hidden_layer)\n",
    "\n",
    "    output_layer = Dense(1,activation='sigmoid')(hidden_layer)\n",
    "\n",
    "    ml_model = Model(inputs=input_layer, outputs=[output_layer])\n",
    "    ml_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    ml_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, verbose=1, batch_size=1024)\n",
    "    \n",
    "    models.append(ml_model)\n",
    "    pred = ml_model.predict(X_test)>0.5\n",
    "    \n",
    "    print(f1_score(pred, y_test, average='macro'))\n",
    "    print(confusion_matrix(pred, y_test))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 162186 samples, validate on 81094 samples\n",
      "Epoch 1/15\n",
      "162186/162186 [==============================] - 47s 292us/step - loss: 0.6224 - acc: 0.8197 - val_loss: 0.3566 - val_acc: 0.8564\n",
      "Epoch 2/15\n",
      "162186/162186 [==============================] - 42s 256us/step - loss: 0.3480 - acc: 0.8572 - val_loss: 0.3413 - val_acc: 0.8565\n",
      "Epoch 3/15\n",
      "162186/162186 [==============================] - 40s 249us/step - loss: 0.3308 - acc: 0.8621 - val_loss: 0.3252 - val_acc: 0.8627\n",
      "Epoch 4/15\n",
      "162186/162186 [==============================] - 42s 256us/step - loss: 0.3243 - acc: 0.8636 - val_loss: 0.3217 - val_acc: 0.8646\n",
      "Epoch 5/15\n",
      "162186/162186 [==============================] - 44s 268us/step - loss: 0.3189 - acc: 0.8652 - val_loss: 0.3161 - val_acc: 0.8664\n",
      "Epoch 6/15\n",
      "162186/162186 [==============================] - 44s 270us/step - loss: 0.3173 - acc: 0.8671 - val_loss: 0.3201 - val_acc: 0.8665\n",
      "Epoch 7/15\n",
      "162186/162186 [==============================] - 41s 253us/step - loss: 0.3149 - acc: 0.8680 - val_loss: 0.3175 - val_acc: 0.8651\n",
      "Epoch 8/15\n",
      "162186/162186 [==============================] - 42s 259us/step - loss: 0.3133 - acc: 0.8689 - val_loss: 0.3137 - val_acc: 0.8667\n",
      "Epoch 9/15\n",
      "162186/162186 [==============================] - 44s 270us/step - loss: 0.3111 - acc: 0.8701 - val_loss: 0.3149 - val_acc: 0.8656\n",
      "Epoch 10/15\n",
      "162186/162186 [==============================] - 44s 274us/step - loss: 0.3115 - acc: 0.8692 - val_loss: 0.3166 - val_acc: 0.8667\n",
      "Epoch 11/15\n",
      "162186/162186 [==============================] - 44s 272us/step - loss: 0.3108 - acc: 0.8696 - val_loss: 0.3126 - val_acc: 0.8679\n",
      "Epoch 12/15\n",
      "162186/162186 [==============================] - 42s 261us/step - loss: 0.3099 - acc: 0.8701 - val_loss: 0.3156 - val_acc: 0.8650\n",
      "Epoch 13/15\n",
      "162186/162186 [==============================] - 42s 261us/step - loss: 0.3088 - acc: 0.8709 - val_loss: 0.3143 - val_acc: 0.8688\n",
      "Epoch 14/15\n",
      "162186/162186 [==============================] - 45s 275us/step - loss: 0.3073 - acc: 0.8719 - val_loss: 0.3139 - val_acc: 0.8676\n",
      "Epoch 15/15\n",
      "162186/162186 [==============================] - 44s 269us/step - loss: 0.3067 - acc: 0.8719 - val_loss: 0.3133 - val_acc: 0.8681\n",
      "Train on 162186 samples, validate on 81094 samples\n",
      "Epoch 1/15\n",
      "162186/162186 [==============================] - 50s 309us/step - loss: 0.6582 - acc: 0.8264 - val_loss: 0.3564 - val_acc: 0.8567\n",
      "Epoch 2/15\n",
      "162186/162186 [==============================] - 43s 266us/step - loss: 0.3412 - acc: 0.8596 - val_loss: 0.3351 - val_acc: 0.8608\n",
      "Epoch 3/15\n",
      "162186/162186 [==============================] - 44s 271us/step - loss: 0.3255 - acc: 0.8628 - val_loss: 0.3269 - val_acc: 0.8613\n",
      "Epoch 4/15\n",
      " 20480/162186 [==>...........................] - ETA: 32s - loss: 0.3192 - acc: 0.8657"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-0157786b42dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_filters=64\n",
    "filter_sizes = [3,3,3]\n",
    "stratified_folder = StratifiedKFold(n_splits=3, random_state=0, shuffle=False) \n",
    "for train_index, test_index in stratified_folder.split(data, y):\n",
    "    \n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "     \n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    embedded_layer = embedding_layer(input_layer)\n",
    "    \n",
    "    reshape = Reshape((MAX_SENTENCE_LENGTH,WORD_VECTOR_DIM, 1))(embedded_layer)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, (filter_sizes[0], WORD_VECTOR_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "    conv_1 = Conv2D(num_filters, (filter_sizes[1], WORD_VECTOR_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "    conv_2 = Conv2D(num_filters, (filter_sizes[2], WORD_VECTOR_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPooling2D((12, 1), strides=(1,1))(conv_0)\n",
    "    maxpool_1 = MaxPooling2D((12, 1), strides=(1,1))(conv_1)\n",
    "    maxpool_2 = MaxPooling2D((12, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "#     conv_0 = Conv2D(num_filters, (filter_sizes[0], WORD_VECTOR_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(maxpool_0)\n",
    "#     conv_1 = Conv2D(num_filters, (filter_sizes[1], WORD_VECTOR_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(maxpool_1)\n",
    "#     conv_2 = Conv2D(num_filters, (filter_sizes[2], WORD_VECTOR_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(maxpool_2)\n",
    "    \n",
    "#     maxpool_0 = MaxPooling2D((2, 1), strides=(1,1))(conv_0)\n",
    "#     maxpool_1 = MaxPooling2D((2, 1), strides=(1,1))(conv_1)\n",
    "#     maxpool_2 = MaxPooling2D((2, 1), strides=(1,1))(conv_2)\n",
    "    \n",
    "    merged_tensor = Concatenate()([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(merged_tensor)\n",
    "#     reshape = Reshape((3*num_filters,))(flatten)\n",
    "    dropout = Dropout(0.5)(flatten) \n",
    "    dropout = Dense(1024)(flatten)\n",
    "    output = Dense(units=1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "    \n",
    "    ml_model = Model(inputs=input_layer, outputs=output)\n",
    "    ml_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    ml_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, verbose=1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8470822365189703\n",
      "[[17146  4432]\n",
      " [ 5390 54126]]\n"
     ]
    }
   ],
   "source": [
    "pred = ml_model.predict(X_test)>0.5\n",
    "\n",
    "print(f1_score(pred, y_test, average='macro'))\n",
    "print(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ml_model.save(\"api_test.h5\")\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['可以爛到這樣也是很猛']*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(map(lambda x : \" \".join(jieba.cut(x.strip() + \" $ENDING$\")), test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(test_sequences, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37695],\n",
       "       [0.37695],\n",
       "       [0.37695],\n",
       "       ...,\n",
       "       [0.37695],\n",
       "       [0.37695],\n",
       "       [0.37695]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model.predict_proba(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "import lightgbm as lgbm\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.3, min_df=0).fit(cut)\n",
    "counts_vector = vectorizer.transform(cut)\n",
    "\n",
    "tfidfVectorizer = TfidfTransformer(smooth_idf=True).fit(counts_vector)\n",
    "tfidf_vector = tfidfVectorizer.transform(counts_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\ttraining's binary_logloss: 0.480827\tvalid_1's binary_logloss: 0.484316\n",
      "[100]\ttraining's binary_logloss: 0.447251\tvalid_1's binary_logloss: 0.451732\n",
      "[150]\ttraining's binary_logloss: 0.42897\tvalid_1's binary_logloss: 0.434317\n",
      "[200]\ttraining's binary_logloss: 0.416446\tvalid_1's binary_logloss: 0.422571\n",
      "[250]\ttraining's binary_logloss: 0.406927\tvalid_1's binary_logloss: 0.413702\n",
      "[300]\ttraining's binary_logloss: 0.399239\tvalid_1's binary_logloss: 0.406634\n",
      "[350]\ttraining's binary_logloss: 0.392771\tvalid_1's binary_logloss: 0.400777\n",
      "[400]\ttraining's binary_logloss: 0.387237\tvalid_1's binary_logloss: 0.395756\n",
      "[450]\ttraining's binary_logloss: 0.382374\tvalid_1's binary_logloss: 0.391295\n",
      "[500]\ttraining's binary_logloss: 0.378057\tvalid_1's binary_logloss: 0.387533\n",
      "[550]\ttraining's binary_logloss: 0.374192\tvalid_1's binary_logloss: 0.384128\n",
      "[600]\ttraining's binary_logloss: 0.370619\tvalid_1's binary_logloss: 0.381029\n",
      "[650]\ttraining's binary_logloss: 0.367381\tvalid_1's binary_logloss: 0.378153\n",
      "[700]\ttraining's binary_logloss: 0.36442\tvalid_1's binary_logloss: 0.375653\n",
      "[750]\ttraining's binary_logloss: 0.361622\tvalid_1's binary_logloss: 0.373224\n",
      "[800]\ttraining's binary_logloss: 0.359061\tvalid_1's binary_logloss: 0.371029\n",
      "[850]\ttraining's binary_logloss: 0.35671\tvalid_1's binary_logloss: 0.369086\n",
      "[900]\ttraining's binary_logloss: 0.354448\tvalid_1's binary_logloss: 0.367131\n",
      "[950]\ttraining's binary_logloss: 0.352316\tvalid_1's binary_logloss: 0.365422\n",
      "[1000]\ttraining's binary_logloss: 0.350291\tvalid_1's binary_logloss: 0.363778\n",
      "[1050]\ttraining's binary_logloss: 0.348412\tvalid_1's binary_logloss: 0.362244\n",
      "[1100]\ttraining's binary_logloss: 0.346602\tvalid_1's binary_logloss: 0.360846\n",
      "[1150]\ttraining's binary_logloss: 0.344842\tvalid_1's binary_logloss: 0.359406\n",
      "[1200]\ttraining's binary_logloss: 0.343221\tvalid_1's binary_logloss: 0.358186\n",
      "[1250]\ttraining's binary_logloss: 0.341679\tvalid_1's binary_logloss: 0.356991\n",
      "[1300]\ttraining's binary_logloss: 0.340184\tvalid_1's binary_logloss: 0.355818\n",
      "[1350]\ttraining's binary_logloss: 0.338746\tvalid_1's binary_logloss: 0.354794\n",
      "[1400]\ttraining's binary_logloss: 0.337369\tvalid_1's binary_logloss: 0.353745\n",
      "[1450]\ttraining's binary_logloss: 0.336067\tvalid_1's binary_logloss: 0.352736\n",
      "[1500]\ttraining's binary_logloss: 0.334816\tvalid_1's binary_logloss: 0.351821\n",
      "[1550]\ttraining's binary_logloss: 0.333588\tvalid_1's binary_logloss: 0.35094\n",
      "[1600]\ttraining's binary_logloss: 0.332389\tvalid_1's binary_logloss: 0.350041\n",
      "[1650]\ttraining's binary_logloss: 0.331231\tvalid_1's binary_logloss: 0.349133\n",
      "[1700]\ttraining's binary_logloss: 0.330127\tvalid_1's binary_logloss: 0.348369\n",
      "[1750]\ttraining's binary_logloss: 0.329049\tvalid_1's binary_logloss: 0.347601\n",
      "[1800]\ttraining's binary_logloss: 0.327983\tvalid_1's binary_logloss: 0.346811\n",
      "[1850]\ttraining's binary_logloss: 0.32696\tvalid_1's binary_logloss: 0.346076\n",
      "[1900]\ttraining's binary_logloss: 0.325968\tvalid_1's binary_logloss: 0.345384\n",
      "[1950]\ttraining's binary_logloss: 0.325004\tvalid_1's binary_logloss: 0.344717\n",
      "[2000]\ttraining's binary_logloss: 0.324093\tvalid_1's binary_logloss: 0.344078\n",
      "[2050]\ttraining's binary_logloss: 0.323158\tvalid_1's binary_logloss: 0.343366\n",
      "[2100]\ttraining's binary_logloss: 0.322271\tvalid_1's binary_logloss: 0.342731\n",
      "[2150]\ttraining's binary_logloss: 0.321438\tvalid_1's binary_logloss: 0.342183\n",
      "[2200]\ttraining's binary_logloss: 0.320616\tvalid_1's binary_logloss: 0.341634\n",
      "[2250]\ttraining's binary_logloss: 0.319789\tvalid_1's binary_logloss: 0.341038\n",
      "[2300]\ttraining's binary_logloss: 0.319016\tvalid_1's binary_logloss: 0.340514\n",
      "[2350]\ttraining's binary_logloss: 0.318249\tvalid_1's binary_logloss: 0.340033\n",
      "[2400]\ttraining's binary_logloss: 0.3175\tvalid_1's binary_logloss: 0.339508\n",
      "[2450]\ttraining's binary_logloss: 0.316755\tvalid_1's binary_logloss: 0.339046\n",
      "[2500]\ttraining's binary_logloss: 0.316048\tvalid_1's binary_logloss: 0.338553\n",
      "[2550]\ttraining's binary_logloss: 0.315342\tvalid_1's binary_logloss: 0.338065\n",
      "[2600]\ttraining's binary_logloss: 0.314657\tvalid_1's binary_logloss: 0.337595\n",
      "[2650]\ttraining's binary_logloss: 0.314005\tvalid_1's binary_logloss: 0.337144\n",
      "[2700]\ttraining's binary_logloss: 0.313344\tvalid_1's binary_logloss: 0.336744\n",
      "[2750]\ttraining's binary_logloss: 0.3127\tvalid_1's binary_logloss: 0.336345\n",
      "[2800]\ttraining's binary_logloss: 0.312071\tvalid_1's binary_logloss: 0.335931\n",
      "[2850]\ttraining's binary_logloss: 0.311459\tvalid_1's binary_logloss: 0.335497\n",
      "[2900]\ttraining's binary_logloss: 0.310859\tvalid_1's binary_logloss: 0.335129\n",
      "[2950]\ttraining's binary_logloss: 0.310272\tvalid_1's binary_logloss: 0.334757\n",
      "[3000]\ttraining's binary_logloss: 0.309691\tvalid_1's binary_logloss: 0.334391\n",
      "[3050]\ttraining's binary_logloss: 0.309115\tvalid_1's binary_logloss: 0.333997\n",
      "[3100]\ttraining's binary_logloss: 0.308565\tvalid_1's binary_logloss: 0.333625\n",
      "[3150]\ttraining's binary_logloss: 0.308017\tvalid_1's binary_logloss: 0.333315\n",
      "[3200]\ttraining's binary_logloss: 0.307477\tvalid_1's binary_logloss: 0.333008\n",
      "[3250]\ttraining's binary_logloss: 0.306943\tvalid_1's binary_logloss: 0.332662\n",
      "[3300]\ttraining's binary_logloss: 0.306429\tvalid_1's binary_logloss: 0.332338\n",
      "[3350]\ttraining's binary_logloss: 0.30592\tvalid_1's binary_logloss: 0.332034\n",
      "[3400]\ttraining's binary_logloss: 0.305393\tvalid_1's binary_logloss: 0.331774\n",
      "[3450]\ttraining's binary_logloss: 0.304895\tvalid_1's binary_logloss: 0.331442\n",
      "[3500]\ttraining's binary_logloss: 0.304423\tvalid_1's binary_logloss: 0.331131\n",
      "[3550]\ttraining's binary_logloss: 0.303949\tvalid_1's binary_logloss: 0.330822\n",
      "[3600]\ttraining's binary_logloss: 0.303469\tvalid_1's binary_logloss: 0.33055\n",
      "[3650]\ttraining's binary_logloss: 0.302997\tvalid_1's binary_logloss: 0.330252\n",
      "[3700]\ttraining's binary_logloss: 0.302542\tvalid_1's binary_logloss: 0.329961\n",
      "[3750]\ttraining's binary_logloss: 0.302094\tvalid_1's binary_logloss: 0.329712\n",
      "[3800]\ttraining's binary_logloss: 0.301672\tvalid_1's binary_logloss: 0.329483\n",
      "[3850]\ttraining's binary_logloss: 0.30123\tvalid_1's binary_logloss: 0.329238\n",
      "[3900]\ttraining's binary_logloss: 0.300808\tvalid_1's binary_logloss: 0.329003\n",
      "[3950]\ttraining's binary_logloss: 0.300374\tvalid_1's binary_logloss: 0.328795\n",
      "[4000]\ttraining's binary_logloss: 0.299958\tvalid_1's binary_logloss: 0.328572\n",
      "[4050]\ttraining's binary_logloss: 0.299538\tvalid_1's binary_logloss: 0.328327\n",
      "[4100]\ttraining's binary_logloss: 0.299133\tvalid_1's binary_logloss: 0.328097\n",
      "[4150]\ttraining's binary_logloss: 0.298716\tvalid_1's binary_logloss: 0.327872\n",
      "[4200]\ttraining's binary_logloss: 0.298327\tvalid_1's binary_logloss: 0.327691\n",
      "[4250]\ttraining's binary_logloss: 0.297944\tvalid_1's binary_logloss: 0.327508\n",
      "[4300]\ttraining's binary_logloss: 0.297532\tvalid_1's binary_logloss: 0.327285\n",
      "[4350]\ttraining's binary_logloss: 0.297116\tvalid_1's binary_logloss: 0.327048\n",
      "[4400]\ttraining's binary_logloss: 0.29675\tvalid_1's binary_logloss: 0.326863\n",
      "[4450]\ttraining's binary_logloss: 0.296383\tvalid_1's binary_logloss: 0.326681\n",
      "[4500]\ttraining's binary_logloss: 0.296001\tvalid_1's binary_logloss: 0.326505\n",
      "[4550]\ttraining's binary_logloss: 0.295635\tvalid_1's binary_logloss: 0.326318\n",
      "[4600]\ttraining's binary_logloss: 0.29528\tvalid_1's binary_logloss: 0.326122\n",
      "[4650]\ttraining's binary_logloss: 0.294911\tvalid_1's binary_logloss: 0.325931\n",
      "[4700]\ttraining's binary_logloss: 0.294576\tvalid_1's binary_logloss: 0.325769\n",
      "[4750]\ttraining's binary_logloss: 0.294229\tvalid_1's binary_logloss: 0.325588\n",
      "[4800]\ttraining's binary_logloss: 0.293863\tvalid_1's binary_logloss: 0.325413\n",
      "[4850]\ttraining's binary_logloss: 0.293515\tvalid_1's binary_logloss: 0.325271\n",
      "[4900]\ttraining's binary_logloss: 0.293174\tvalid_1's binary_logloss: 0.325116\n",
      "[4950]\ttraining's binary_logloss: 0.292818\tvalid_1's binary_logloss: 0.324954\n",
      "[5000]\ttraining's binary_logloss: 0.292485\tvalid_1's binary_logloss: 0.324802\n",
      "[5050]\ttraining's binary_logloss: 0.292166\tvalid_1's binary_logloss: 0.324669\n",
      "[5100]\ttraining's binary_logloss: 0.291843\tvalid_1's binary_logloss: 0.32451\n",
      "[5150]\ttraining's binary_logloss: 0.291519\tvalid_1's binary_logloss: 0.324353\n",
      "[5200]\ttraining's binary_logloss: 0.291179\tvalid_1's binary_logloss: 0.324232\n",
      "[5250]\ttraining's binary_logloss: 0.290895\tvalid_1's binary_logloss: 0.324079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5300]\ttraining's binary_logloss: 0.290589\tvalid_1's binary_logloss: 0.323912\n",
      "[5350]\ttraining's binary_logloss: 0.290286\tvalid_1's binary_logloss: 0.323771\n",
      "[5400]\ttraining's binary_logloss: 0.289982\tvalid_1's binary_logloss: 0.323643\n",
      "[5450]\ttraining's binary_logloss: 0.289677\tvalid_1's binary_logloss: 0.323542\n",
      "[5500]\ttraining's binary_logloss: 0.289377\tvalid_1's binary_logloss: 0.323424\n",
      "[5550]\ttraining's binary_logloss: 0.289097\tvalid_1's binary_logloss: 0.323329\n",
      "[5600]\ttraining's binary_logloss: 0.288811\tvalid_1's binary_logloss: 0.323181\n",
      "[5650]\ttraining's binary_logloss: 0.288537\tvalid_1's binary_logloss: 0.323048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-47b22b03da31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stratified_folder = StratifiedKFold(n_splits=4, random_state=0, shuffle=False) \n",
    "for train_index, test_index in stratified_folder.split(tfidf_vector, y): \n",
    "    X_train, X_test = tfidf_vector[train_index], tfidf_vector[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "    \n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_classes':1,\n",
    "        'subsample': .9,\n",
    "        'colsample_bytree': .5,\n",
    "        'reg_alpha': .1,\n",
    "        'reg_lambda': .1,\n",
    "        'min_split_gain': 0.001,\n",
    "        'min_child_weight': 5,\n",
    "        'silent':True,\n",
    "        'verbosity':-1,\n",
    "        'n_jobs':6\n",
    "    }\n",
    "\n",
    "    clf = lgbm.LGBMClassifier(**lgb_params,learning_rate=0.05,\n",
    "                                    n_estimators=10000,max_depth=4)\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "        verbose=50,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    pred = clf.predict(X_test, num_iteration=clf.best_iteration_)\n",
    "    print(f1_score(pred, y_test, average='macro'))\n",
    "    print(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '爛'\n",
    "test = \" \".join(jieba.cut(test))\n",
    "test = tfidfVectorizer.transform(vectorizer.transform([test]))\n",
    "model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
