{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from datetime import datetime as dt\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "import scipy\n",
    "import seaborn\n",
    "import jieba\n",
    "# jieba.enable_parallel(4)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryRating(rating):\n",
    "    return 1 if rating > 3 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = MongoClient('10.160.1.15:27017')['googleplay']['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(collection.find({},{'comments':1,'ratings':1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"google_play.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['comments','ratings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = df.apply(lambda x:len(str(x['comments']).strip()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(wordCount>=4) & (df['ratings']!='3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = df[['comments']].apply(lambda x: \" \".join(jieba.cut(x['comments'].strip() + \" $ENDING$\",cut_all=False)),axis=1)\n",
    "# cut = df[['comments']].apply(lambda x: \" \".join(x['comments']) + \" $ENDING$\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_corpus = WikiCorpus('zhwiki-20190320-pages-articles-multistream.xml.bz2', dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已處理 1000 篇文章\n",
      "已處理 2000 篇文章\n",
      "已處理 3000 篇文章\n",
      "已處理 4000 篇文章\n",
      "已處理 5000 篇文章\n",
      "已處理 6000 篇文章\n",
      "已處理 7000 篇文章\n",
      "已處理 8000 篇文章\n",
      "已處理 9000 篇文章\n",
      "已處理 10000 篇文章\n",
      "已處理 11000 篇文章\n",
      "已處理 12000 篇文章\n",
      "已處理 13000 篇文章\n",
      "已處理 14000 篇文章\n",
      "已處理 15000 篇文章\n",
      "已處理 16000 篇文章\n",
      "已處理 17000 篇文章\n",
      "已處理 18000 篇文章\n",
      "已處理 19000 篇文章\n",
      "已處理 20000 篇文章\n",
      "已處理 21000 篇文章\n",
      "已處理 22000 篇文章\n",
      "已處理 23000 篇文章\n",
      "已處理 24000 篇文章\n",
      "已處理 25000 篇文章\n",
      "已處理 26000 篇文章\n",
      "已處理 27000 篇文章\n",
      "已處理 28000 篇文章\n",
      "已處理 29000 篇文章\n",
      "已處理 30000 篇文章\n",
      "已處理 31000 篇文章\n",
      "已處理 32000 篇文章\n",
      "已處理 33000 篇文章\n",
      "已處理 34000 篇文章\n",
      "已處理 35000 篇文章\n",
      "已處理 36000 篇文章\n",
      "已處理 37000 篇文章\n",
      "已處理 38000 篇文章\n",
      "已處理 39000 篇文章\n",
      "已處理 40000 篇文章\n",
      "已處理 41000 篇文章\n",
      "已處理 42000 篇文章\n",
      "已處理 43000 篇文章\n",
      "已處理 44000 篇文章\n",
      "已處理 45000 篇文章\n",
      "已處理 46000 篇文章\n",
      "已處理 47000 篇文章\n",
      "已處理 48000 篇文章\n",
      "已處理 49000 篇文章\n",
      "已處理 50000 篇文章\n",
      "已處理 51000 篇文章\n",
      "已處理 52000 篇文章\n",
      "已處理 53000 篇文章\n",
      "已處理 54000 篇文章\n",
      "已處理 55000 篇文章\n",
      "已處理 56000 篇文章\n",
      "已處理 57000 篇文章\n",
      "已處理 58000 篇文章\n",
      "已處理 59000 篇文章\n",
      "已處理 60000 篇文章\n",
      "已處理 61000 篇文章\n",
      "已處理 62000 篇文章\n",
      "已處理 63000 篇文章\n",
      "已處理 64000 篇文章\n",
      "已處理 65000 篇文章\n",
      "已處理 66000 篇文章\n",
      "已處理 67000 篇文章\n",
      "已處理 68000 篇文章\n",
      "已處理 69000 篇文章\n",
      "已處理 70000 篇文章\n",
      "已處理 71000 篇文章\n",
      "已處理 72000 篇文章\n",
      "已處理 73000 篇文章\n",
      "已處理 74000 篇文章\n",
      "已處理 75000 篇文章\n",
      "已處理 76000 篇文章\n",
      "已處理 77000 篇文章\n",
      "已處理 78000 篇文章\n",
      "已處理 79000 篇文章\n",
      "已處理 80000 篇文章\n",
      "已處理 81000 篇文章\n",
      "已處理 82000 篇文章\n",
      "已處理 83000 篇文章\n",
      "已處理 84000 篇文章\n",
      "已處理 85000 篇文章\n",
      "已處理 86000 篇文章\n",
      "已處理 87000 篇文章\n",
      "已處理 88000 篇文章\n",
      "已處理 89000 篇文章\n",
      "已處理 90000 篇文章\n",
      "已處理 91000 篇文章\n",
      "已處理 92000 篇文章\n",
      "已處理 93000 篇文章\n",
      "已處理 94000 篇文章\n",
      "已處理 95000 篇文章\n",
      "已處理 96000 篇文章\n",
      "已處理 97000 篇文章\n",
      "已處理 98000 篇文章\n",
      "已處理 99000 篇文章\n",
      "已處理 100000 篇文章\n",
      "已處理 101000 篇文章\n",
      "已處理 102000 篇文章\n",
      "已處理 103000 篇文章\n",
      "已處理 104000 篇文章\n",
      "已處理 105000 篇文章\n",
      "已處理 106000 篇文章\n",
      "已處理 107000 篇文章\n",
      "已處理 108000 篇文章\n",
      "已處理 109000 篇文章\n",
      "已處理 110000 篇文章\n",
      "已處理 111000 篇文章\n",
      "已處理 112000 篇文章\n",
      "已處理 113000 篇文章\n",
      "已處理 114000 篇文章\n",
      "已處理 115000 篇文章\n",
      "已處理 116000 篇文章\n",
      "已處理 117000 篇文章\n",
      "已處理 118000 篇文章\n",
      "已處理 119000 篇文章\n",
      "已處理 120000 篇文章\n",
      "已處理 121000 篇文章\n",
      "已處理 122000 篇文章\n",
      "已處理 123000 篇文章\n",
      "已處理 124000 篇文章\n",
      "已處理 125000 篇文章\n",
      "已處理 126000 篇文章\n",
      "已處理 127000 篇文章\n",
      "已處理 128000 篇文章\n",
      "已處理 129000 篇文章\n",
      "已處理 130000 篇文章\n",
      "已處理 131000 篇文章\n",
      "已處理 132000 篇文章\n",
      "已處理 133000 篇文章\n",
      "已處理 134000 篇文章\n",
      "已處理 135000 篇文章\n",
      "已處理 136000 篇文章\n",
      "已處理 137000 篇文章\n",
      "已處理 138000 篇文章\n",
      "已處理 139000 篇文章\n",
      "已處理 140000 篇文章\n",
      "已處理 141000 篇文章\n",
      "已處理 142000 篇文章\n",
      "已處理 143000 篇文章\n",
      "已處理 144000 篇文章\n",
      "已處理 145000 篇文章\n",
      "已處理 146000 篇文章\n",
      "已處理 147000 篇文章\n",
      "已處理 148000 篇文章\n",
      "已處理 149000 篇文章\n",
      "已處理 150000 篇文章\n",
      "已處理 151000 篇文章\n",
      "已處理 152000 篇文章\n",
      "已處理 153000 篇文章\n",
      "已處理 154000 篇文章\n",
      "已處理 155000 篇文章\n",
      "已處理 156000 篇文章\n",
      "已處理 157000 篇文章\n",
      "已處理 158000 篇文章\n",
      "已處理 159000 篇文章\n",
      "已處理 160000 篇文章\n",
      "已處理 161000 篇文章\n",
      "已處理 162000 篇文章\n",
      "已處理 163000 篇文章\n",
      "已處理 164000 篇文章\n",
      "已處理 165000 篇文章\n",
      "已處理 166000 篇文章\n",
      "已處理 167000 篇文章\n",
      "已處理 168000 篇文章\n",
      "已處理 169000 篇文章\n",
      "已處理 170000 篇文章\n",
      "已處理 171000 篇文章\n",
      "已處理 172000 篇文章\n",
      "已處理 173000 篇文章\n",
      "已處理 174000 篇文章\n",
      "已處理 175000 篇文章\n",
      "已處理 176000 篇文章\n",
      "已處理 177000 篇文章\n",
      "已處理 178000 篇文章\n",
      "已處理 179000 篇文章\n",
      "已處理 180000 篇文章\n",
      "已處理 181000 篇文章\n",
      "已處理 182000 篇文章\n",
      "已處理 183000 篇文章\n",
      "已處理 184000 篇文章\n",
      "已處理 185000 篇文章\n",
      "已處理 186000 篇文章\n",
      "已處理 187000 篇文章\n",
      "已處理 188000 篇文章\n",
      "已處理 189000 篇文章\n",
      "已處理 190000 篇文章\n",
      "已處理 191000 篇文章\n",
      "已處理 192000 篇文章\n",
      "已處理 193000 篇文章\n",
      "已處理 194000 篇文章\n",
      "已處理 195000 篇文章\n",
      "已處理 196000 篇文章\n",
      "已處理 197000 篇文章\n",
      "已處理 198000 篇文章\n",
      "已處理 199000 篇文章\n",
      "已處理 200000 篇文章\n",
      "已處理 201000 篇文章\n",
      "已處理 202000 篇文章\n",
      "已處理 203000 篇文章\n",
      "已處理 204000 篇文章\n",
      "已處理 205000 篇文章\n",
      "已處理 206000 篇文章\n",
      "已處理 207000 篇文章\n",
      "已處理 208000 篇文章\n",
      "已處理 209000 篇文章\n",
      "已處理 210000 篇文章\n",
      "已處理 211000 篇文章\n",
      "已處理 212000 篇文章\n",
      "已處理 213000 篇文章\n",
      "已處理 214000 篇文章\n",
      "已處理 215000 篇文章\n",
      "已處理 216000 篇文章\n",
      "已處理 217000 篇文章\n",
      "已處理 218000 篇文章\n",
      "已處理 219000 篇文章\n",
      "已處理 220000 篇文章\n",
      "已處理 221000 篇文章\n",
      "已處理 222000 篇文章\n",
      "已處理 223000 篇文章\n",
      "已處理 224000 篇文章\n",
      "已處理 225000 篇文章\n",
      "已處理 226000 篇文章\n",
      "已處理 227000 篇文章\n",
      "已處理 228000 篇文章\n",
      "已處理 229000 篇文章\n",
      "已處理 230000 篇文章\n",
      "已處理 231000 篇文章\n",
      "已處理 232000 篇文章\n",
      "已處理 233000 篇文章\n",
      "已處理 234000 篇文章\n",
      "已處理 235000 篇文章\n",
      "已處理 236000 篇文章\n",
      "已處理 237000 篇文章\n",
      "已處理 238000 篇文章\n",
      "已處理 239000 篇文章\n",
      "已處理 240000 篇文章\n",
      "已處理 241000 篇文章\n",
      "已處理 242000 篇文章\n"
     ]
    }
   ],
   "source": [
    "texts_num = 0\n",
    "with open(\"wiki_zh_tw.txt\",'w',encoding='utf-8') as output:\n",
    "    for text in df['comments']:\n",
    "        output.write(text + '\\n')\n",
    "        texts_num += 1\n",
    "        if texts_num % 1000 == 0:\n",
    "            print(\"已處理 %d 篇文章\" % texts_num)\n",
    "    output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成前 10000 行的斷詞\n",
      "已完成前 20000 行的斷詞\n",
      "已完成前 30000 行的斷詞\n",
      "已完成前 40000 行的斷詞\n",
      "已完成前 50000 行的斷詞\n",
      "已完成前 60000 行的斷詞\n",
      "已完成前 70000 行的斷詞\n",
      "已完成前 80000 行的斷詞\n",
      "已完成前 90000 行的斷詞\n",
      "已完成前 100000 行的斷詞\n",
      "已完成前 110000 行的斷詞\n",
      "已完成前 120000 行的斷詞\n",
      "已完成前 130000 行的斷詞\n",
      "已完成前 140000 行的斷詞\n",
      "已完成前 150000 行的斷詞\n",
      "已完成前 160000 行的斷詞\n",
      "已完成前 170000 行的斷詞\n",
      "已完成前 180000 行的斷詞\n",
      "已完成前 190000 行的斷詞\n",
      "已完成前 200000 行的斷詞\n",
      "已完成前 210000 行的斷詞\n",
      "已完成前 220000 行的斷詞\n",
      "已完成前 230000 行的斷詞\n",
      "已完成前 240000 行的斷詞\n"
     ]
    }
   ],
   "source": [
    "output = open('wiki_seg.txt', 'w', encoding='utf-8')\n",
    "with open('wiki_zh_tw.txt', 'r', encoding='utf-8') as content :\n",
    "    for texts_num, line in enumerate(content):\n",
    "        line = line.strip('\\n')\n",
    "        words = jieba.cut(line, cut_all=False)\n",
    "        for word in words:\n",
    "#             if word not in stopword_set:\n",
    "            output.write(word + ' ')\n",
    "        output.write('\\n')\n",
    "\n",
    "        if (texts_num + 1) % 10000 == 0:\n",
    "            print(\"已完成前 %d 行的斷詞\" % (texts_num + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "WORD_VECTOR_DIM = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "# Create a model to represent each word by a 10 dimensional vector.\n",
    "sentences = models.word2vec.Text8Corpus(cut[0])\n",
    "\n",
    "sentences = word2vec.LineSentence(\"wiki_seg.txt\")\n",
    "wvmodel = word2vec.Word2Vec(sentences, size=250)\n",
    "wvmodel = Word2Vec(sentences, size=WORD_VECTOR_DIM, window=5, min_count=0, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, Flatten, Dropout, GRU, BatchNormalization\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 135230 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=1000000000)\n",
    "tokenizer.fit_on_texts(cut)\n",
    "sequences = tokenizer.texts_to_sequences(cut)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140825 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for word in wvmodel.wv.vocab:\n",
    "    coefs = np.asarray(wvmodel.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, WORD_VECTOR_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            WORD_VECTOR_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENTENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['ratings'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i] = 1 if y[i] > 3 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = Sequential()\n",
    "ml_model.add(embedding_layer)\n",
    "ml_model.add(GRU(32, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(8, return_sequences=True))\n",
    "ml_model.add(Flatten())\n",
    "ml_model.add(Dropout(0.5))\n",
    "# model.add(Dense(32))\n",
    "ml_model.add(Dense(1))\n",
    "\n",
    "ml_model.add(Activation('sigmoid'))\n",
    "ml_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181635 samples, validate on 60547 samples\n",
      "Epoch 1/10\n",
      "181635/181635 [==============================] - 47s 258us/step - loss: 0.3672 - acc: 0.8328 - val_loss: 0.3221 - val_acc: 0.8566\n",
      "Epoch 2/10\n",
      "181635/181635 [==============================] - 54s 299us/step - loss: 0.3133 - acc: 0.8624 - val_loss: 0.3087 - val_acc: 0.8618\n",
      "Epoch 3/10\n",
      "181635/181635 [==============================] - 53s 293us/step - loss: 0.3020 - acc: 0.8674 - val_loss: 0.3013 - val_acc: 0.8696\n",
      "Epoch 4/10\n",
      "181635/181635 [==============================] - 54s 300us/step - loss: 0.2958 - acc: 0.8716 - val_loss: 0.2959 - val_acc: 0.8711\n",
      "Epoch 5/10\n",
      "181635/181635 [==============================] - 51s 282us/step - loss: 0.2919 - acc: 0.8729 - val_loss: 0.2973 - val_acc: 0.8683\n",
      "Epoch 6/10\n",
      "181635/181635 [==============================] - 50s 277us/step - loss: 0.2883 - acc: 0.8753 - val_loss: 0.2950 - val_acc: 0.8711\n",
      "Epoch 7/10\n",
      "181635/181635 [==============================] - 60s 330us/step - loss: 0.2852 - acc: 0.8763 - val_loss: 0.2898 - val_acc: 0.8739\n",
      "Epoch 8/10\n",
      "181635/181635 [==============================] - 65s 358us/step - loss: 0.2834 - acc: 0.8768 - val_loss: 0.2883 - val_acc: 0.8749\n",
      "Epoch 9/10\n",
      "181635/181635 [==============================] - 51s 282us/step - loss: 0.2815 - acc: 0.8784 - val_loss: 0.2882 - val_acc: 0.8751\n",
      "Epoch 10/10\n",
      "181635/181635 [==============================] - 45s 246us/step - loss: 0.2798 - acc: 0.8793 - val_loss: 0.2871 - val_acc: 0.8760\n",
      "0.8466160363119336\n",
      "[[13273  3957]\n",
      " [ 3552 39765]]\n"
     ]
    }
   ],
   "source": [
    "stratified_folder = StratifiedKFold(n_splits=4, random_state=0, shuffle=False) \n",
    "for train_index, test_index in stratified_folder.split(data, y):\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "    ml_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=1, batch_size=256)\n",
    "    pred = ml_model.predict_classes(X_test)\n",
    "    print(f1_score(pred, y_test, average='macro'))\n",
    "    print(confusion_matrix(pred, y_test))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ml_model.save(\"api_test.h5\")\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['沒有一次登入成功的 可以爛到這樣也是很猛']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(map(lambda x : \" \".join(jieba.cut(x.strip() + \" $ENDING$\")), test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-63c1ea387f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             warnings.warn('Network returning invalid probability values. '\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ml_model.predict_proba(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "import lightgbm as lgbm\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.3, min_df=0).fit(cut)\n",
    "counts_vector = vectorizer.transform(cut)\n",
    "\n",
    "tfidfVectorizer = TfidfTransformer(smooth_idf=True).fit(counts_vector)\n",
    "tfidf_vector = tfidfVectorizer.transform(counts_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.480021\tvalid_1's binary_logloss: 0.485176\n",
      "[100]\tvalid_0's binary_logloss: 0.447456\tvalid_1's binary_logloss: 0.455579\n",
      "[150]\tvalid_0's binary_logloss: 0.428476\tvalid_1's binary_logloss: 0.438524\n",
      "[200]\tvalid_0's binary_logloss: 0.415527\tvalid_1's binary_logloss: 0.426968\n",
      "[250]\tvalid_0's binary_logloss: 0.40567\tvalid_1's binary_logloss: 0.418001\n",
      "[300]\tvalid_0's binary_logloss: 0.397815\tvalid_1's binary_logloss: 0.411123\n",
      "[350]\tvalid_0's binary_logloss: 0.391154\tvalid_1's binary_logloss: 0.405144\n",
      "[400]\tvalid_0's binary_logloss: 0.385503\tvalid_1's binary_logloss: 0.40022\n",
      "[450]\tvalid_0's binary_logloss: 0.380543\tvalid_1's binary_logloss: 0.395811\n",
      "[500]\tvalid_0's binary_logloss: 0.376171\tvalid_1's binary_logloss: 0.392053\n",
      "[550]\tvalid_0's binary_logloss: 0.372194\tvalid_1's binary_logloss: 0.388535\n",
      "[600]\tvalid_0's binary_logloss: 0.368598\tvalid_1's binary_logloss: 0.385468\n",
      "[650]\tvalid_0's binary_logloss: 0.365273\tvalid_1's binary_logloss: 0.382771\n",
      "[700]\tvalid_0's binary_logloss: 0.362278\tvalid_1's binary_logloss: 0.380248\n",
      "[750]\tvalid_0's binary_logloss: 0.359482\tvalid_1's binary_logloss: 0.377914\n",
      "[800]\tvalid_0's binary_logloss: 0.356846\tvalid_1's binary_logloss: 0.37565\n",
      "[850]\tvalid_0's binary_logloss: 0.354438\tvalid_1's binary_logloss: 0.373768\n",
      "[900]\tvalid_0's binary_logloss: 0.352125\tvalid_1's binary_logloss: 0.371827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-47b22b03da31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    740\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    540\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stratified_folder = StratifiedKFold(n_splits=4, random_state=0, shuffle=False) \n",
    "for train_index, test_index in stratified_folder.split(tfidf_vector, y): \n",
    "    X_train, X_test = tfidf_vector[train_index], tfidf_vector[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "    \n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_classes':1,\n",
    "        'subsample': .9,\n",
    "        'colsample_bytree': .5,\n",
    "        'reg_alpha': .1,\n",
    "        'reg_lambda': .1,\n",
    "        'min_split_gain': 0.001,\n",
    "        'min_child_weight': 5,\n",
    "        'silent':True,\n",
    "        'verbosity':-1,\n",
    "        'nthread':2\n",
    "    }\n",
    "\n",
    "    clf = lgbm.LGBMClassifier(**lgb_params,learning_rate=0.05,\n",
    "                                    n_estimators=10000,max_depth=4)\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "        verbose=50,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    pred = clf.predict(X_test, num_iteration=clf.best_iteration_)\n",
    "    print(f1_score(pred, y_test, average='macro'))\n",
    "    print(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '爛'\n",
    "test = \" \".join(jieba.cut(test))\n",
    "test = tfidfVectorizer.transform(vectorizer.transform([test]))\n",
    "model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
